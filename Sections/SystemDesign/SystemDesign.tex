\section{Environment and actions}
In contrast to other approaches on a 2D lattice, I will instead 
model the environment using a 3D FCC lattice, which has the added benefit of
being computationally trivial to model. I make use of the
matrix representation:
\begin{equation}
    \begin{split}
        & \mathbf{e} = \begin{bmatrix}
            0 & 0.5 & 0.5 \\
            0.5 & 0 & 0.5 \\
            0.5 & 0.5 & 0
        \end{bmatrix} \\
    \end{split}
\end{equation}
Any point in space is represented as a linear combination of the
column space of the bravaise lattice, within a discrete action setting,
this can be viewed as a choice among $3^3$ possible actions $ a = \begin{bmatrix}
    x \\ y \\ z
\end{bmatrix}$ s.t $(x,y,z) \in \{-1,0,1 \}$.
The chains of amino acids are initialised as a straight line in space, and each
time-step each residue moves to another position $x_t$ by selecting a movement vector
$a_{\mathbf{j}}$ to be applied to it's current position. The transition function
is characterised as:
\begin{equation}
    T(x_t, a_{\mathbf{j}}) =\mathbf{e} \cdot a_{\mathbf{j}}
\end{equation}
In order to preserve constraints between neighbouring resisues on the backbone, each agent
continues to take steps in the environment until a \emph{legal} action in made, otherwise
being punishived for violating the self avoiding walk, occupying active sites and occupying a lattice site that is 
too far from neighbouring residues i.e if the distance between residues with index $(i-1, i)$
and $(i, i+1)$ is not an element of $\{\sqrt{0.5}, 1\}$ in an FCC configuration, then the backbone
of the chain has been broken. Once a permitted action is taken, the agent is rewarded accordingly,
then transitioned into an \emph{absorbing state} with reward 0 to preserve the gurantees of finite MDPs\footnote{Modified from \cite{Mguni2018}'s construction of spatial congestion games}.
Each episode consists of one round of agents sequentially taking actions until a terminal state is reached,
in which case the end state of the previous episode is passed as the starting state of the next episode. The
system is therefore evaluated for a finite number of timesteps $\backsim 1e6$ and halted when an equilibrium
state has been reached, indicating no more permissible moves are available, or any that are taken will only negatively
impact the global reward.
\section{Revising reward structures}
Instead of using the standard rewards associated with the $HP$ model, 
I will instead divide each of the residues into their respective categories
under the $hHPNX$ model with the signs of values inverted, each residue acts as their own agent with their own
reward function determined according to the column space in table (2.3). In
addition to these rewards, the agents are further penalized with with a reward
of -10 for occupying the same space as another residue to discourage overlapping residues:
\begin{table}[!htb]
    \caption{Reward structure}
    \begin{center}
        \caption{}
        \begin{tabular}{|c || c | c | c | c | c|}
            \hline
             & h & H & P & N & X \\
            \hline
            h & -2 & 4 & 0 & 0 & 0 \\
            \hline
            H & 4 & 3 & 0 & 0 & 0 \\
            \hline
            P & 0 & 0 & -1 & 1 & 0\\
            \hline
            N & 0 & 0 & 1 & -1 & 0\\
            \hline
            X & 0 & 0 & 0 & 0 & 0\\ 
            \hline
            $x' = x$ & -10 & -10 & -10 & -10 & -10\\
            \hline
        \end{tabular}
    \end{center}
\end{table}\\
\subsection{Potential based reward shaping}
In multi-agent settings, altering the structure of rewards has been shown to 
encourage faster convergence to an optimal joint policy \cite{Devlin2014}.
The \emph{difference reward} restructures the reward such that agent is rewarded
based on their contribution to the global optimization goal, this does not
require expert knowledge. \emph{Potential based reward
shaping} instead shapes the reward according to a potential function that encodes the desireability
of certain states crafted by expert knowledge. \cite{Devlin2014} unify these two domains
under \emph{difference rewards incorporating potential-based reward shaping (DRiP} in order to gain the mutual advantages of both methods.\\

Firstly, the local reward is defined as $L_j$, this is the reward for the immediate transition
that has just occured $(s,a,s',r)$. The global reward is the sum over all the local rewards of all the agents
$G = \sum_J L_j$.
\begin{itemize}
    \item This difference reward is defined as: \[D_i(z) = G(z) - G(z^{-j})\] where $z$ indicates
    the collective states or state-action pairs of the agents.
    \item The potential based reward shaping is defined by \[PBRS = L_j + \gamma \boldsymbol \Phi(s') - \boldsymbol \Phi(s)\]
    where $\boldsymbol \Phi(s)$ encodes the intrisic desireability of the state and $\gamma$ is the same discount
    factor used by the agent. 
    \begin{displayquote}[\cite{Devlin2014}]
        This formulation of reward shaping has been proven to not
        alter the optimal policy of a single agent in both infinite- andfinite- state MDPs. 
    \end{displayquote}
\end{itemize}
\cite{Devlin2014} describe the generic form of a shaped reward (4.3) and formulate \emph{DRiP} (4.4) within this framework:
\begin{equation}
    \begin{gathered}
        r_{shaped} = r(s,a,s') + F(s,s')\\
        \text{Where $r(s,a,s') = L_k$ and $F(s,s')$ is the additional shaping function}
    \end{gathered}
\end{equation}
\begin{equation}
    \begin{gathered}
        r(s,a,s') = G(s,a,s')\\
        F(s,s') = -G(s'^{-j}) + \gamma \boldsymbol \Phi(s') - \boldsymbol \Phi(s)
    \end{gathered}
\end{equation}
I take the global reward to be the sum of the rewards of all contacts on lattice
sites centered at each agent's position, the desireability of occupying a site
is given by the spatial congestion loss defined by \cite{Mguni2018} for $\alpha < 0$
favouring sites with greater density. The joint state $m_{x_t}$ of a site is given
by the fraction of neighbouring residues on a particular site. In an FCC configuration,
a given site can have a maximum of $n = 12$ neighbours, so if the occupation of a site
results in the central residue taking on 12 neighbours, the site is said to be maximally dense.
\begin{equation}
    \begin{gathered}
        \boldsymbol \Phi(m_{x_t}, x_t) = \frac{\exp (-(x_t - \mu)^\top \Sigma^{-1} (x_t - \mu))}{2 \pi \sqrt{\vert \Sigma \vert}(1 + m_{x_t})^\alpha}
    \end{gathered}
\end{equation}
This heterogenous reward structure given by the hHPNX categorisation
is afforded by the gurantees provided by \cite{Sriram2020},
which ensures convergence with lower bounds within the mean-field scheme when considering 
multiple types of agents.                
\section{Mean field multi-type spatial congestion games}
As suggested previously, each residue is modelled as an individual agent belonging to one of the
categories as listed in the hHPNX model. The reward structure of each type is heterogenous and so can be considered
to be members of disjoint types. I model the inter-domain cooperativity by positively rewarding
the agents for the occupation of lattice sites with:
\begin{itemize}
    \item Favourable contacts with neighbouring residues 
    \item Dense occupation of other residues
\end{itemize}
Rewards for the occupation of dense areas of space encourages cooperativity amongst all
agents and acts as a guide to the most compact states. The reward for favourable contacts
also filters the landscape of possible conformations leaving only those that maximise
the number of hydrophic contacts while being optimally compact, this mirrors the
work of \cite{Yang} regarding the two tiers of interactions. The \emph{DRiP} reward attempts to
shape the reward lanscape similarly to the \emph{Gibbs energy funnel}, the addition
of the potential shaping ensures that agents do not encounter significantly sparse rewards
and can respond to a consistent training signal.
The use of the \emph{hHPNX} scheme on a 3D FCC lattice also reduces the number of degenerative 
results as \cite{Hoque} shows.\\

Each agent's Q function is updated in accordance with the multi-type paradigm which was shown to be provably convergent:
\begin{equation}
    Q^j_{t+1}(s,a^j,a^{-j}, a^{-j}_1, \hdots , a^{-j}_M)= (1-\alpha)Q^j_t(s,a^j,a^{-j}, a^{-j}_1, \hdots , a^{-j}_M) +\alpha[R^j(m_{x_t}, x_t, a^j, \gamma)+ \gamma v^j_t(s')]
\end{equation}
Where $a^j$ represents the action taken by the central agent, $a^{-j}$ and $a^{-j}_m$ represents
the empirical distribution of all neighbours and neighbours of type $m$ respectively.
\section{Risk sensitive agents}
As \cite{Xueguang2018} show, distributional reinforcement learning proves to be effective
in multi-agent settings; an agent is better able to \emph{distribute blame} proportionally
among the actions of neighbouring agents. For instance, for a given transition $s\rightarrow s'$,
if we consider the joint action taken by all agents $\mathbf{a}$, then a central agent $j$ may determine
that the action taken by a specific agent $\mathbf{a}_k$ was the most influential factor in the
transition to specific state $s'$; this is also known as the \emph{credit assignment problem}. In their work, they use quantile networks to determine a state-specific learning rate $\alpha$
that changes based on the interaction with other agents, becoming more or less exploratory
as the situation deems.\\

I considered this approach, aswell as an alternative approach that shaped the the reward accord to the
predicted distribution of of returns for the actions selected by neighbours. However I instead argue that
due to the structure of the reward, the quantile returns that the agents are modelling are that of their
\textbf{expected contribution} to the global goal and so no changes to the loss function
presented by \cite{Sriram2020} are required. It is relevant to question whether the combination
of the techniques presented here still converge to the same optimal joint policy $\mathbf{\pi}*$ or 
to the same set of nash equilibrium strategies, although a more thorough analysis is required,
I present the following reasoning as to why the agents will still converge to the same set of
nash strategies:
\begin{itemize}
    \item \cite{Yang2018} prove that $\exists \: \boldsymbol \pi^* \backsim \{\pi_k^{MF}\}$, there exists
        an optimal joint policy when the policies of each agent are evaluated according to the
        mean field approximation
    \item \cite{Devlin2014} prove that the construction of the potential difference reward \textbf{does not
    alter the underlying nash equilibrium strategies}.
    \item \cite{Dabney2017} prove that the repeated application of the distributional bellman operator 
    $\mathcal{T}^\pi$ converges to a fixed point among all policies for an individual agent.
\end{itemize}
In summary, there exists an optimal joint strategy $\boldsymbol \pi^*  \triangleq [\pi^1, \pi^2, \hdots, \pi^N]$ in a mean field game,
the shaping of the reward will not change this optimal joint strategy. Each agent is tasked with predicting
it's own contribution to this global optimum, by modelling the contribution
to a fixed point rather than total returns, each agent is guranteed to converge to a fixed point that
maximizes it's contribution to the global goal determined by local interactions. This alleviates much of the instability associated with 
the non-stationarity of the environment from the perspective of each agent. 
I now present the combination of the methods, the equations are presented for the
standard Mean-Field Q learning update, and are straightforwardly extended to the 
multi-type variation.
First, dueling quantile networks are used to estimate the quantiles for each action:
\begin{equation}
    \begin{gathered}
        V(s) \coloneqq \theta_V \backsim Z_{\theta_V} \\
        A(s,a^j, a^{-j}) \coloneqq \theta_{A}^a  \backsim Z_{\theta_A}\;\; \text{where } a \in \{1,2, \hdots, \vert \mathbf{A} \vert \} \\
        \hat{Z}(s,a^j, a^{-j}) = \theta_V + (\theta_{A}^a - \mathbb{E}_{a \backsim \mathbf{A}}[\theta_{A}^{a}])
    \end{gathered}
\end{equation}
The sum over the element-wise product of the output quantile distributions of actions and their
corresponding quantile weight is then taken, this is expectation of the expected returns
$\E_{Z_{\theta}}[\hat{Z}(s,a^j, a^{-j})]$, the expectation of these actions is then used to construct the 
\emph{Boltzmann} distribution.
The product of this boltzmann weighting, and the empircal probability of the actions
of neighbouring residues is then used to weight the original output distribution as in 
equations (2.74-76):
\begin{equation}
    \pi^j_t(a^j \mid s',a^{-j}) = \frac{\exp(- \beta \E[\hat{Z}(s,a^j, a^{-j})])}{\sum_{a^j \in \mathbf{A}^j} \exp(- \beta \E[\hat{Z}(s,a^j, a^{-j})])}
\end{equation}
\begin{equation}
    Z^*(s,a^j, a^{-j}) = \pi^j_t(a^j \mid s',a^{-j}) \cdot \mathbb{E}_{a^j \backsim(\mathbf{a}^{-j}), Z_\theta \backsim Z}[\hat{Z}(s,a^j, a^{-j})]
\end{equation}
The sum of $Z^*(s,a^j, a^{-j})$ over all actions for agent $j$ produced the mean-field value $v^{MF}_t$ of that state as 
in equation 2.76.,this is done for both the behaviour network $\theta$ and the target network $\theta'$.
The huber loss is then calculated against the target as in equation 2.64,
prioritised replay importance sampling ratios are then applied before back propagation::
\begin{equation}
    \begin{gathered}
        \mathcal{L}(\theta^j) = \sum^N_{i=1} \E_{Z^* \backsim Z}[\rho_{\tau}(Z^*_{\theta'} - Z^*_{\theta})] \\
    \nabla_\theta E = w_j \cdot \mathcal{L}(\theta^j) \cdot \nabla_\theta Z^*(s,a^j, a^{-j}\: ; \: \theta)
    \end{gathered}
\end{equation}

\section{Implementation}
The learning agents have been implemented using the Pytorch
machine learning framework whereas the environment was implemented
solely using Numpy in order to remain agnostic of the underlying machine
learning framework as well as to promotehigh levels of interoperability
with other frameworks and projects as Numpy is a common Python package.

The folder structure of the project is as follows:
\begin{itemize}
    \item Agent
    \begin{itemize}
        \item This contains various agents used for development and testing
        including a Rainbow Quantile agent implemented using a regular MLP,
        another Rainbow Quantile agent used for training on Atari in order
        to verify the implementation. The final agent is the Reside Agent
        which has been adapted from the Rainbow Quantile to work specifically
        with the mean field training regime.
    \end{itemize}
    \item Environment
    \begin{itemize}
        \item This contains the code for the lattice environment as well
        as the reward structures according to the hHPNX model.
    \end{itemize}
    \item Memory
    \begin{itemize}
        \item This contains the prioritised replay memory module
        used for all agents.
    \end{itemize}
    \item MultiAgent
    \begin{itemize}
        \item This directory holds the main classes used for training:
        \begin{itemize}
            \item \texttt{GlobalBuffer}

            This stores the neighbourhood information for all agents and is
            used to maintain a cache of previously selected actions and to 
            calculate action distributions for use during training.
            \item \texttt{MultiAgentRoutine}

            This class contains the core training loop for all agents as well
            as the initialisation logic. All parameters can be passed in here,
            key word arguments that are not explicitly named (\texttt{**kwargs})
            in the class' constructor are passed directly into the agents for 
            easy configuration. 

            At initialisation, when the \texttt{GlobalBuffer} is empty, random 
            neighbour action distributions are used just as in the original
            implementation by \cite{Yang2018}.
        \end{itemize}
    \end{itemize}
    \item Network
    \begin{itemize}
        \item This directory contains various neural networks that were
        used to verify my implementation of deep Q learning throughout the
        project. In particular, the \texttt{ResidueNetwork} was adapted
        from the the \texttt{QauntileNetwork} to perform additional
        steps of computation such as the action distribution embedding
        and the concatenation of the outputs of various layers. 
    \end{itemize}
    \item Peptides
    \begin{itemize}
        \item This contains the output \texttt{.json} files that can be 
        used to visualise outputs during training.
    \end{itemize}
    \item PongVideos 
    \begin{itemize}
        \item This directory contains output videos from training
        the \texttt{QuantileAgent} on the OpenAI Gym
        Atari suite in order to verify the correctness of the implementation.
    \end{itemize}
    \item \texttt{DisplayProtein.ipynb}
    \begin{itemize}
        \item This top level file contains the example code to visualise
        the output peptides.
    \end{itemize}
    \item \texttt{run\_quantile.py}
    \begin{itemize}
        \item This contains the code used to train the \texttt{QuantilAtariAgent}.
    \end{itemize}
\end{itemize}

The final hyperparameters for the project are provided as follows:

\begin{table}[!htb]
    \begin{center}
        \caption{Hyperparameters}
\begin{tabular}{||c | c||}
    \hline
    Hyperparameter & Value\\
    \hline\hline
    Total episodes  & 100,000  \\ 
    \hline
    Prioritised experience replay $\beta$ frames & 10,000  \\
    \hline
    Target update frequency & 5 \\
    \hline
    Steps before training & 5000  \\
    \hline
    $\gamma$ & 0.95 \\
    \hline
    Agent memory buffer size & 10000 \\
    \hline
    Memory batch size & 128 \\
    \hline
    Neural network hidden layer size & 512 \\
    \hline
    Action embedding layer size & $64 \times 32$ \\
    \hline
    Number of quantles & 51 \\
    \hline
    Mean field $\beta$ at initialisation & 1 \\
    \hline
    Mean field $\beta$ final value & 0.1 \\
    \hline
    Mean field $\tau$ & 0.005 \\
    \hline
\end{tabular}
\end{center}
\end{table}