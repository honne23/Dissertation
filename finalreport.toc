\contentsline {chapter}{\numberline {1}Introduction}{12}{chapter.1}%
\contentsline {section}{\numberline {1.1}Problem Background}{12}{section.1.1}%
\contentsline {section}{\numberline {1.2}Overview}{13}{section.1.2}%
\contentsline {section}{\numberline {1.3}Project Aims and Objectives}{14}{section.1.3}%
\contentsline {section}{\numberline {1.4}Success Criteria}{14}{section.1.4}%
\contentsline {section}{\numberline {1.5}Structure of Report}{15}{section.1.5}%
\contentsline {chapter}{\numberline {2}Literature Review}{17}{chapter.2}%
\contentsline {section}{\numberline {2.1}Proteins}{17}{section.2.1}%
\contentsline {subsection}{\numberline {2.1.1}Amino Acids \& Poly-Peptides}{17}{subsection.2.1.1}%
\contentsline {subsection}{\numberline {2.1.2}Protein Structures}{18}{subsection.2.1.2}%
\contentsline {subsection}{\numberline {2.1.3}The Protein's Energy Landscape}{20}{subsection.2.1.3}%
\contentsline {subsection}{\numberline {2.1.4}Bioinformatics}{23}{subsection.2.1.4}%
\contentsline {subsubsection}{\numberline {2.1.4.1}Homology Modelling}{23}{subsubsection.2.1.4.1}%
\contentsline {subsubsection}{\numberline {2.1.4.2}Lattice Models}{24}{subsubsection.2.1.4.2}%
\contentsline {subsubsection}{\numberline {2.1.4.3}Bravais Lattices}{24}{subsubsection.2.1.4.3}%
\contentsline {subsubsection}{\numberline {2.1.4.4}HP Model}{25}{subsubsection.2.1.4.4}%
\contentsline {subsubsection}{\numberline {2.1.4.5}hHPNX Model}{28}{subsubsection.2.1.4.5}%
\contentsline {section}{\numberline {2.2}Reinforcement Learning}{29}{section.2.2}%
\contentsline {subsection}{\numberline {2.2.1}Markov Decision Processes}{29}{subsection.2.2.1}%
\contentsline {subsubsection}{\numberline {2.2.1.1}Bellman Equation}{32}{subsubsection.2.2.1.1}%
\contentsline {subsubsection}{\numberline {2.2.1.2}Optimality}{33}{subsubsection.2.2.1.2}%
\contentsline {subsubsection}{\numberline {2.2.1.3}Generalised Policy Iteration}{33}{subsubsection.2.2.1.3}%
\contentsline {subsubsection}{\numberline {2.2.1.4}Exploration vs Exploitation}{35}{subsubsection.2.2.1.4}%
\contentsline {subsubsection}{\numberline {2.2.1.5}Monte Carlo Estimation}{35}{subsubsection.2.2.1.5}%
\contentsline {subsubsection}{\numberline {2.2.1.6}Model Free vs Model Based}{37}{subsubsection.2.2.1.6}%
\contentsline {subsection}{\numberline {2.2.2}Deep Q Learning}{38}{subsection.2.2.2}%
\contentsline {subsubsection}{\numberline {2.2.2.1}Temporal Difference Error}{38}{subsubsection.2.2.2.1}%
\contentsline {subsubsection}{\numberline {2.2.2.2}Q Learning}{38}{subsubsection.2.2.2.2}%
\contentsline {subsubsection}{\numberline {2.2.2.3}Neural Networks}{38}{subsubsection.2.2.2.3}%
\contentsline {subsubsection}{\numberline {2.2.2.4}Deep Q-Networks}{41}{subsubsection.2.2.2.4}%
\contentsline {subsection}{\numberline {2.2.3}Improvements to Vanilla Deep Q-Networks}{43}{subsection.2.2.3}%
\contentsline {subsubsection}{\numberline {2.2.3.1}Prioritised Experience Replay}{44}{subsubsection.2.2.3.1}%
\contentsline {subsubsection}{\numberline {2.2.3.2}Dueling Networks}{46}{subsubsection.2.2.3.2}%
\contentsline {subsubsection}{\numberline {2.2.3.3}Double Q Learning }{46}{subsubsection.2.2.3.3}%
\contentsline {subsubsection}{\numberline {2.2.3.4}Distributional Reinforcement Learning}{48}{subsubsection.2.2.3.4}%
\contentsline {section}{\numberline {2.3}Multi-Agent Reinforcement Learning}{52}{section.2.3}%
\contentsline {subsection}{\numberline {2.3.1}Stochastic Games}{52}{subsection.2.3.1}%
\contentsline {subsection}{\numberline {2.3.2}Mean Field Games}{54}{subsection.2.3.2}%
\contentsline {subsection}{\numberline {2.3.3}Spatial Congestion Games}{56}{subsection.2.3.3}%
\contentsline {subsection}{\numberline {2.3.4}Mean Field Multi Type Reinforcement Learning}{57}{subsection.2.3.4}%
\contentsline {section}{\numberline {2.4}Related Work}{58}{section.2.4}%
\contentsline {subsection}{\numberline {2.4.1}MCMC \& Integer Programming methods for lattice models}{58}{subsection.2.4.1}%
\contentsline {subsection}{\numberline {2.4.2}Deep Learning methods for protein structure prediction}{60}{subsection.2.4.2}%
\contentsline {subsubsection}{\numberline {2.4.2.1}Alpha-Fold}{61}{subsubsection.2.4.2.1}%
\contentsline {subsection}{\numberline {2.4.3}Deep Reinforcement Learning methods for lattice models}{62}{subsection.2.4.3}%
\contentsline {subsection}{\numberline {2.4.4}Multi-agent structure prediction methods}{63}{subsection.2.4.4}%
\contentsline {chapter}{\numberline {3}System Requirements and Specification}{65}{chapter.3}%
\contentsline {section}{\numberline {3.1}Drawbacks of previous approaches}{65}{section.3.1}%
\contentsline {section}{\numberline {3.2}Requirements}{66}{section.3.2}%
\contentsline {subsection}{\numberline {3.2.1}Functional Requirements}{67}{subsection.3.2.1}%
\contentsline {subsection}{\numberline {3.2.2}Non functional requirements}{67}{subsection.3.2.2}%
\contentsline {section}{\numberline {3.3}Experimental Procedure}{68}{section.3.3}%
\contentsline {chapter}{\numberline {4}System Design and Analysis}{69}{chapter.4}%
\contentsline {section}{\numberline {4.1}Environment and actions}{69}{section.4.1}%
\contentsline {section}{\numberline {4.2}Revising reward structures}{70}{section.4.2}%
\contentsline {subsection}{\numberline {4.2.1}Potential based reward shaping}{71}{subsection.4.2.1}%
\contentsline {section}{\numberline {4.3}Mean field multi-type spatial congestion games}{72}{section.4.3}%
\contentsline {section}{\numberline {4.4}Risk sensitive agents}{73}{section.4.4}%
\contentsline {chapter}{\numberline {5}Evaluation \& Testing}{76}{chapter.5}%
\contentsline {section}{\numberline {5.1}Experiment Setup}{76}{section.5.1}%
\contentsline {section}{\numberline {5.2}Results}{77}{section.5.2}%
\contentsline {subsection}{\numberline {5.2.1}Environment}{77}{subsection.5.2.1}%
\contentsline {subsection}{\numberline {5.2.2}Quantile Rainbow Agent}{78}{subsection.5.2.2}%
\contentsline {subsection}{\numberline {5.2.3}Mean Field Multi-Agent Learning}{78}{subsection.5.2.3}%
