import torchimport numpy as npimport gymimport torch.optim as optimimport torch.nn.functional as Ffrom torch.nn.utils import clip_grad_norm_from Network.DuelingNetwork import DuelingNetworkfrom Memory.PrioritisedReplay import PrioritisedReplayclass DDQN:        def __init__(self,                 env: gym,                 hidden_size:int = 128,                 max_epsilon: float = 1.0,                 min_epsilon: float = 0.01,                 epsilon_decay: float = (1/2000),                 mem_size: int = 5000,                 batch_size: int = 32,                 gamma: float = 0.99,                 lr: float = 1e-3                 ):        self.env = env        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'        self.dqn = DuelingNetwork(self.env.observation_space.shape[0], hidden_size, self.env.action_space.n).to(self.device)        self.target = DuelingNetwork(self.env.observation_space.shape[0], hidden_size, self.env.action_space.n).to(self.device)        self.target.eval()        self.target.load_state_dict(self.dqn.state_dict())        self.optimizer = optim.Adam(params= self.dqn.parameters())        self.max_epsilon = max_epsilon        self.epsilon = max_epsilon        self.min_epsilon = min_epsilon        self.epsilon_decay = epsilon_decay        self.batch_size = batch_size        self.memory = PrioritisedReplay(mem_size)        self.gamma = gamma                def update_network(self) -> float:                idx, weights, samples = self.memory.sample(self.batch_size)        weights = torch.FloatTensor(weights.astype(float)).to(self.device)                samples = np.vstack(samples)        state, action, reward, next_state, done = samples.T        state = torch.FloatTensor(np.vstack(state)).to(self.device)        next_state = torch.FloatTensor(np.vstack(next_state)).to(self.device)        action = torch.LongTensor(action.astype(int).reshape(-1, 1)).to(self.device)        reward = torch.FloatTensor(reward.astype(float).reshape(-1, 1)).to(self.device)        done = torch.FloatTensor(done.astype(bool).reshape(-1, 1)).to(self.device)                curr_q_value = self.dqn(state).gather(1, action)        selections = self.dqn(next_state).argmax(1).view(-1,1)        next_q_value = self.target(next_state).gather(1,selections).detach()        mask = 1 - done        target = (reward + self.gamma * next_q_value * mask).to(self.device)                loss = F.smooth_l1_loss(curr_q_value, target, reduction="none")        self.memory.update(idx, loss.detach().cpu().numpy())                loss = (loss * weights.view(-1,1)).mean()        self.optimizer.zero_grad()        loss.backward()        clip_grad_norm_(self.dqn.parameters(), 10.0)                self.optimizer.step()                return loss.item()        def update_epsilon(self):        self.epsilon = max( \                    self.min_epsilon, self.epsilon - ( \                        self.max_epsilon - self.min_epsilon \                    ) * self.epsilon_decay \                )    def update_beta(self, frame_idx:int, num_frames:int):        fraction = min(frame_idx / num_frames, 1.0)        beta = self.memory.beta         beta = beta + fraction * (1.0 - beta)        self.memory.beta = beta                    def select_action(self, state) -> int:        if self.epsilon > np.random.random():            selected_action = self.env.action_space.sample()        else:            with torch.no_grad():                state = torch.FloatTensor(state).to(self.device)                selected_action = self.dqn(state).cpu().argmax().item()        return selected_action        def step(self, state: np.array) -> tuple:        action = self.select_action(state)        next_state, reward, done, _ = self.env.step(action)        transition = [state, action, reward, next_state, done]        self.memory.store(transition)        return next_state, reward, done        def target_update(self):        self.target.load_state_dict(self.dqn.state_dict())