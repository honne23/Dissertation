\BOOKMARK [0][-]{chapter.1}{Introduction}{}% 1
\BOOKMARK [1][-]{section.1.1}{Problem Background}{chapter.1}% 2
\BOOKMARK [2][-]{subsection.1.1.1}{What is protein folding?}{section.1.1}% 3
\BOOKMARK [2][-]{subsection.1.1.2}{The analogue approach}{section.1.1}% 4
\BOOKMARK [2][-]{subsection.1.1.3}{Why is this a difficult problem?}{section.1.1}% 5
\BOOKMARK [1][-]{section.1.2}{Overview}{chapter.1}% 6
\BOOKMARK [1][-]{section.1.3}{Project Aims and Objectives}{chapter.1}% 7
\BOOKMARK [1][-]{section.1.4}{Success Criteria}{chapter.1}% 8
\BOOKMARK [1][-]{section.1.5}{Structure of Report}{chapter.1}% 9
\BOOKMARK [0][-]{chapter.2}{Literature Review}{}% 10
\BOOKMARK [1][-]{section.2.1}{Proteins}{chapter.2}% 11
\BOOKMARK [2][-]{subsection.2.1.1}{Amino Acids \046 Poly-Peptides}{section.2.1}% 12
\BOOKMARK [2][-]{subsection.2.1.2}{Protein Structures}{section.2.1}% 13
\BOOKMARK [2][-]{subsection.2.1.3}{The Protein's Energy Landscape}{section.2.1}% 14
\BOOKMARK [2][-]{subsection.2.1.4}{Bioinformatics}{section.2.1}% 15
\BOOKMARK [3][-]{subsubsection.2.1.4.1}{Homology Modelling}{subsection.2.1.4}% 16
\BOOKMARK [3][-]{subsubsection.2.1.4.2}{Lattice Models}{subsection.2.1.4}% 17
\BOOKMARK [3][-]{subsubsection.2.1.4.3}{Bravais Lattices}{subsection.2.1.4}% 18
\BOOKMARK [3][-]{subsubsection.2.1.4.4}{HP Model}{subsection.2.1.4}% 19
\BOOKMARK [3][-]{subsubsection.2.1.4.5}{hHPNX Model}{subsection.2.1.4}% 20
\BOOKMARK [1][-]{section.2.2}{Reinforcement Learning}{chapter.2}% 21
\BOOKMARK [2][-]{subsection.2.2.1}{Markov Decision Processes}{section.2.2}% 22
\BOOKMARK [3][-]{subsubsection.2.2.1.1}{Bellman Equation}{subsection.2.2.1}% 23
\BOOKMARK [3][-]{subsubsection.2.2.1.2}{Optimality}{subsection.2.2.1}% 24
\BOOKMARK [3][-]{subsubsection.2.2.1.3}{Generalised Policy Iteration}{subsection.2.2.1}% 25
\BOOKMARK [3][-]{subsubsection.2.2.1.4}{Exploration vs Exploitation}{subsection.2.2.1}% 26
\BOOKMARK [3][-]{subsubsection.2.2.1.5}{Monte Carlo Estimation}{subsection.2.2.1}% 27
\BOOKMARK [3][-]{subsubsection.2.2.1.6}{Model Free vs Model Based}{subsection.2.2.1}% 28
\BOOKMARK [2][-]{subsection.2.2.2}{Deep Q Learning}{section.2.2}% 29
\BOOKMARK [3][-]{subsubsection.2.2.2.1}{Temporal Difference Error}{subsection.2.2.2}% 30
\BOOKMARK [3][-]{subsubsection.2.2.2.2}{Q Learning}{subsection.2.2.2}% 31
\BOOKMARK [3][-]{subsubsection.2.2.2.3}{Neural Networks}{subsection.2.2.2}% 32
\BOOKMARK [3][-]{subsubsection.2.2.2.4}{Deep Q-Networks}{subsection.2.2.2}% 33
\BOOKMARK [2][-]{subsection.2.2.3}{Improvements to Vanilla Deep Q-Networks}{section.2.2}% 34
\BOOKMARK [3][-]{subsubsection.2.2.3.1}{Prioritised Experience Replay}{subsection.2.2.3}% 35
\BOOKMARK [3][-]{subsubsection.2.2.3.2}{Dueling Networks}{subsection.2.2.3}% 36
\BOOKMARK [3][-]{subsubsection.2.2.3.3}{Double Q Learning }{subsection.2.2.3}% 37
\BOOKMARK [3][-]{subsubsection.2.2.3.4}{Distributional Reinforcement Learning}{subsection.2.2.3}% 38
\BOOKMARK [1][-]{section.2.3}{Multi-Agent Reinforcement Learning}{chapter.2}% 39
\BOOKMARK [2][-]{subsection.2.3.1}{Stochastic Games}{section.2.3}% 40
\BOOKMARK [2][-]{subsection.2.3.2}{Mean Field Games}{section.2.3}% 41
\BOOKMARK [2][-]{subsection.2.3.3}{Spatial Congestion Games}{section.2.3}% 42
\BOOKMARK [2][-]{subsection.2.3.4}{Mean Field Multi Type Reinforcement Learning}{section.2.3}% 43
\BOOKMARK [1][-]{section.2.4}{Related Work}{chapter.2}% 44
\BOOKMARK [2][-]{subsection.2.4.1}{MCMC \046 Integer Programming methods for lattice models}{section.2.4}% 45
\BOOKMARK [2][-]{subsection.2.4.2}{Deep Learning methods for protein structure prediction}{section.2.4}% 46
\BOOKMARK [3][-]{subsubsection.2.4.2.1}{Alpha-Fold}{subsection.2.4.2}% 47
\BOOKMARK [2][-]{subsection.2.4.3}{Deep Reinforcement Learning methods for lattice models}{section.2.4}% 48
\BOOKMARK [2][-]{subsection.2.4.4}{Multi-agent structure prediction methods}{section.2.4}% 49
\BOOKMARK [0][-]{chapter.3}{System Requirements and Specification}{}% 50
\BOOKMARK [1][-]{section.3.1}{Drawbacks of previous approaches}{chapter.3}% 51
\BOOKMARK [1][-]{section.3.2}{Requirements}{chapter.3}% 52
\BOOKMARK [2][-]{subsection.3.2.1}{Functional Requirements}{section.3.2}% 53
\BOOKMARK [2][-]{subsection.3.2.2}{Non functional requirements}{section.3.2}% 54
\BOOKMARK [1][-]{section.3.3}{Experimental Procedure}{chapter.3}% 55
\BOOKMARK [0][-]{chapter.4}{System Design and Analysis}{}% 56
\BOOKMARK [1][-]{section.4.1}{Environment and actions}{chapter.4}% 57
\BOOKMARK [1][-]{section.4.2}{Revising reward structures}{chapter.4}% 58
\BOOKMARK [2][-]{subsection.4.2.1}{Potential based reward shaping}{section.4.2}% 59
\BOOKMARK [1][-]{section.4.3}{Mean field multi-type spatial congestion games}{chapter.4}% 60
\BOOKMARK [1][-]{section.4.4}{Risk sensitive agents}{chapter.4}% 61
\BOOKMARK [1][-]{section.4.5}{Implementation}{chapter.4}% 62
\BOOKMARK [0][-]{chapter.5}{Evaluation \046 Testing}{}% 63
\BOOKMARK [1][-]{section.5.1}{Experiment Setup}{chapter.5}% 64
\BOOKMARK [1][-]{section.5.2}{Results}{chapter.5}% 65
\BOOKMARK [2][-]{subsection.5.2.1}{Environment}{section.5.2}% 66
\BOOKMARK [2][-]{subsection.5.2.2}{Quantile Rainbow Agent}{section.5.2}% 67
\BOOKMARK [2][-]{subsection.5.2.3}{Mean Field Multi-Agent Learning}{section.5.2}% 68
\BOOKMARK [0][-]{chapter.6}{Discussion \046 Conclusion}{}% 69
\BOOKMARK [1][-]{section.6.1}{Discussion}{chapter.6}% 70
\BOOKMARK [1][-]{section.6.2}{Future Work}{chapter.6}% 71
\BOOKMARK [1][-]{section.6.3}{Conclusion}{chapter.6}% 72
\BOOKMARK [0][-]{chapter.7}{Statement of Ethics}{}% 73
\BOOKMARK [1][-]{section.7.1}{Personal Data}{chapter.7}% 74
\BOOKMARK [1][-]{section.7.2}{Moral Considerations}{chapter.7}% 75
\BOOKMARK [1][-]{section.7.3}{Copyright}{chapter.7}% 76
